"""
ç‹¬ç«‹æ¨¡å‹è¯„ä¼°è„šæœ¬ - Stanford UGVR CVé¡¹ç›®
ç”¨äºè¯„ä¼°è®­ç»ƒå¥½çš„CLIPSegæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½
"""

import os
import sys
import json
import argparse
from pathlib import Path
from typing import Dict, List, Tuple
import numpy as np
import pandas as pd
from PIL import Image
import torch
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from config import *
from utils import (
    FineTuneDataset, 
    create_data_loader, 
    load_model, 
    calculate_iou,
    evaluate_model
)

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°ï¼ˆä½¿ç”¨config.pyä¸­çš„é»˜è®¤å€¼ï¼‰"""
    parser = argparse.ArgumentParser(description='CLIPSegæ¨¡å‹è¯„ä¼°è„šæœ¬')
    
    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼
    parser.add_argument('--model_path', type=str, 
                        default=str(EVALUATION_CONFIG['default_model_path']),
                        help='è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--test_data', type=str, 
                        default=str(EVALUATION_CONFIG['default_test_data']),
                        help='æµ‹è¯•æ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--output_dir', type=str, 
                        default=str(EVALUATION_CONFIG['default_output_dir']),
                        help='è¯„ä¼°ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--batch_size', type=int, 
                        default=EVALUATION_CONFIG['batch_size'],
                        help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--num_samples', type=int, 
                        default=EVALUATION_CONFIG['num_visualization_samples'],
                        help='å¯è§†åŒ–æ ·æœ¬æ•°é‡')
    parser.add_argument('--device', type=str, 
                        default=EVALUATION_CONFIG['device'],
                        help='è®¡ç®—è®¾å¤‡ (auto/cpu/cuda/mps)')
    
    return parser.parse_args()

def setup_device(device_arg: str) -> torch.device:
    """è®¾ç½®è®¡ç®—è®¾å¤‡"""
    if device_arg == 'auto':
        if torch.cuda.is_available():
            device = torch.device('cuda')
        elif torch.backends.mps.is_available():
            device = torch.device('mps')
        else:
            device = torch.device('cpu')
    else:
        device = torch.device(device_arg)
    
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    return device

def load_test_data(test_data_dir: str, batch_size: int, processor) -> Tuple[DataLoader, int]:
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    test_images = sorted(Path(test_data_dir).glob('*.tif'))
    test_images = [str(img) for img in test_images]
    
    if not test_images:
        raise ValueError(f"åœ¨ {test_data_dir} ä¸­æœªæ‰¾åˆ°æµ‹è¯•å›¾åƒ")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    test_loader = create_data_loader(
        test_images, 
        str(VAIHINGEN_LABELS_DIR), 
        URBAN_CLASSES, 
        processor,
        batch_size,
        shuffle=False
    )
    
    print(f"åŠ è½½äº† {len(test_images)} å¼ æµ‹è¯•å›¾åƒ")
    return test_loader, len(test_images)

def evaluate_detailed_performance(model, test_loader, device, classes) -> Dict:
    """è¯¦ç»†æ€§èƒ½è¯„ä¼°"""
    model.eval()
    all_predictions = []
    all_targets = []
    class_ious = {cls: [] for cls in classes}
    
    print("å¼€å§‹è¯¦ç»†è¯„ä¼°...")
    
    with torch.no_grad():
        for batch in tqdm(test_loader, desc="è¯„ä¼°è¿›åº¦"):
            # æ¨¡å‹é¢„æµ‹
            outputs = model(
                pixel_values=batch["pixel_values"].to(device),
                input_ids=batch["input_ids"].to(device),
                attention_mask=batch["attention_mask"].to(device)
            )
            logits = outputs.logits
            labels = batch["labels"].to(device)
            
            # ç¡®ä¿logitså’Œlabelsçš„ç»´åº¦åŒ¹é…
            if logits.dim() == 3 and labels.dim() == 4:
                logits = logits.unsqueeze(1)
            elif logits.dim() == 4 and labels.dim() == 4:
                pass  # ç»´åº¦å·²ç»åŒ¹é…
            else:
                print(f"è­¦å‘Š: logitså½¢çŠ¶={logits.shape}, labelså½¢çŠ¶={labels.shape}")
                
            predictions = torch.sigmoid(logits)
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            pred_np = predictions.cpu().numpy()
            target_np = labels.cpu().numpy()
            
            # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„IoU
            for i, cls in enumerate(classes):
                pred_binary = (pred_np[:, i] > 0.5).astype(int)
                target_binary = target_np[:, i].astype(int)
                
                # è®¡ç®—IoU
                iou = calculate_iou(pred_binary, target_binary)
                class_ious[cls].append(iou)
                
                # æ”¶é›†é¢„æµ‹å’Œç›®æ ‡ç”¨äºæ··æ·†çŸ©é˜µ
                all_predictions.extend(pred_binary.flatten())
                all_targets.extend(target_binary.flatten())
    
    # è®¡ç®—å¹³å‡IoU
    mean_ious = {}
    for cls in classes:
        mean_ious[cls] = np.mean(class_ious[cls]) if class_ious[cls] else 0.0
    
    # æ€»ä½“mIoU
    mean_iou = np.mean(list(mean_ious.values()))
    
    results = {
        'class_ious': mean_ious,
        'mean_iou': mean_iou,
        'predictions': all_predictions,
        'targets': all_targets
    }
    
    return results

def generate_performance_report(results: Dict, classes: List[str], output_dir: Path):
    """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # 1. IoUç»“æœè¡¨æ ¼
    iou_df = pd.DataFrame([
        {'Class': cls, 'IoU': iou} 
        for cls, iou in results['class_ious'].items()
    ])
    iou_df.loc[len(iou_df)] = {'Class': 'Mean', 'IoU': results['mean_iou']}
    
    # ä¿å­˜IoUç»“æœ
    iou_df.to_csv(output_dir / 'iou_results.csv', index=False)
    
    # 2. æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(results['targets'], results['predictions'])
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('æ··æ·†çŸ©é˜µ')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.tight_layout()
    plt.savefig(output_dir / 'confusion_matrix.png', dpi=300)
    plt.close()
    
    # 3. ç±»åˆ«æ€§èƒ½å¯è§†åŒ–
    plt.figure(figsize=(12, 8))
    bars = plt.bar(results['class_ious'].keys(), results['class_ious'].values())
    plt.axhline(y=results['mean_iou'], color='red', linestyle='--', 
                label=f'Mean IoU: {results["mean_iou"]:.3f}')
    plt.title('å„ç±»åˆ«IoUæ€§èƒ½')
    plt.ylabel('IoUåˆ†æ•°')
    plt.xlabel('ç±»åˆ«')
    plt.xticks(rotation=45, ha='right')
    plt.legend()
    plt.tight_layout()
    plt.savefig(output_dir / 'class_performance.png', dpi=300)
    plt.close()
    
    # 4. è¯¦ç»†æŠ¥å‘Š
    report = {
        'evaluation_summary': {
            'mean_iou': results['mean_iou'],
            'class_ious': results['class_ious'],
            'total_samples': len(results['targets'])
        },
        'evaluation_time': pd.Timestamp.now().isoformat()
    }
    
    with open(output_dir / 'evaluation_report.json', 'w') as f:
        json.dump(report, f, indent=2)
    
    return report

def visualize_predictions(model_path: str, test_data_dir: str, device, classes, 
                         output_dir: Path, num_samples: int = 10):
    """å¯è§†åŒ–é¢„æµ‹ç»“æœ - å‚è€ƒclipseg_run.pyçš„4é¢æ¿å¯è§†åŒ–æ–¹å¼"""
    from transformers import CLIPSegProcessor, CLIPSegForImageSegmentation
    import tifffile
    
    # åŠ è½½é¢„è®­ç»ƒå’Œå¾®è°ƒæ¨¡å‹ç”¨äºå¯¹æ¯”
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    processor_pretrained = CLIPSegProcessor.from_pretrained(PRETRAINED_MODEL)
    model_pretrained = CLIPSegForImageSegmentation.from_pretrained(PRETRAINED_MODEL)
    model_pretrained.to(device)
    model_pretrained.eval()
    
    # åŠ è½½å¾®è°ƒæ¨¡å‹
    processor_finetuned = CLIPSegProcessor.from_pretrained(model_path)
    model_finetuned = CLIPSegForImageSegmentation.from_pretrained(model_path)
    model_finetuned.to(device)
    model_finetuned.eval()
    
    # åˆ†å‰²é¢œè‰²æ˜ å°„
    SEGMENTATION_COLORS_RGB = {
        "impervious surface": (255, 255, 255),  # White
        "building":           (0, 0, 255),      # Blue
        "low vegetation":     (0, 255, 255),    # Cyan
        "tree":               (0, 255, 0),      # Green
        "car":                (255, 255, 0),    # Yellow
        "background":         (255, 0, 0)       # Red
    }
    
    # è·å–æµ‹è¯•å›¾åƒ
    test_images = sorted(Path(test_data_dir).glob('*.tif'))
    if len(test_images) > num_samples:
        import random
        random.seed(42)
        test_images = random.sample(test_images, num_samples)
    
    vis_dir = output_dir / 'visualizations'
    vis_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"ç”Ÿæˆ {len(test_images)} ä¸ªå¯è§†åŒ–æ ·æœ¬...")
    
    for i, image_path in enumerate(tqdm(test_images, desc="ç”Ÿæˆå¯è§†åŒ–")):
        # å¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶
        label_path = VAIHINGEN_LABELS_DIR / image_path.name
        
        if not Path(label_path).exists():
            print(f"æ ‡ç­¾æ–‡ä»¶ä¸å­˜åœ¨: {label_path}")
            continue
            
        try:
            # åŠ è½½å›¾åƒ
            image = Image.open(image_path).convert("RGB")
            images = [image for _ in classes]
            
            # é¢„è®­ç»ƒæ¨¡å‹æ¨ç†
            inputs_pretrained = processor_pretrained(images=images, text=classes, return_tensors="pt", padding=True)
            inputs_pretrained = {k: v.to(device) for k, v in inputs_pretrained.items()}
            with torch.no_grad():
                outputs_pretrained = model_pretrained(**inputs_pretrained)
            masks_pretrained = outputs_pretrained.logits.sigmoid().squeeze().cpu().numpy()
            resized_masks_pretrained = [
                np.array(Image.fromarray(m).resize(image.size, resample=Image.BILINEAR))
                for m in masks_pretrained
            ]
            
            # å¾®è°ƒæ¨¡å‹æ¨ç†
            inputs_finetuned = processor_finetuned(images=images, text=classes, return_tensors="pt", padding=True)
            inputs_finetuned = {k: v.to(device) for k, v in inputs_finetuned.items()}
            with torch.no_grad():
                outputs_finetuned = model_finetuned(**inputs_finetuned)
            masks_finetuned = outputs_finetuned.logits.sigmoid().squeeze().cpu().numpy()
            resized_masks_finetuned = [
                np.array(Image.fromarray(m).resize(image.size, resample=Image.BILINEAR))
                for m in masks_finetuned
            ]
            
            # ç”Ÿæˆåˆ†å‰²å›¾
            color_map_rgb = np.array([SEGMENTATION_COLORS_RGB[prompt] for prompt in classes], dtype=np.uint8)
            
            # é¢„è®­ç»ƒåˆ†å‰²å›¾
            stacked_masks_pretrained = np.stack(resized_masks_pretrained, axis=0)
            pred_labels_pretrained = np.argmax(stacked_masks_pretrained, axis=0)
            segmentation_map_pretrained = color_map_rgb[pred_labels_pretrained]
            
            # å¾®è°ƒåˆ†å‰²å›¾
            stacked_masks_finetuned = np.stack(resized_masks_finetuned, axis=0)
            pred_labels_finetuned = np.argmax(stacked_masks_finetuned, axis=0)
            segmentation_map_finetuned = color_map_rgb[pred_labels_finetuned]
            
            # åŠ è½½çœŸå®æ ‡ç­¾
            gt_img_bgr = tifffile.imread(label_path)
            
            # åˆ›å»º4é¢æ¿å¯è§†åŒ– - ä½¿ç”¨é…ç½®ä¸­çš„è®¾ç½®
            fig, axes = plt.subplots(2, 2, 
                                   figsize=EVALUATION_CONFIG['figure_size'], 
                                   dpi=EVALUATION_CONFIG['visualization_dpi'])
            
            # åŸå§‹å›¾åƒ
            axes[0, 0].imshow(np.array(image))
            axes[0, 0].set_title(f"Original Image\n{image_path.name}", fontsize=12)
            axes[0, 0].axis("off")
            
            # é¢„è®­ç»ƒæ¨¡å‹è¾“å‡º
            axes[0, 1].imshow(segmentation_map_pretrained)
            axes[0, 1].set_title(f"Pretrained CLIPSeg Output", fontsize=12)
            axes[0, 1].axis("off")
            
            # å¾®è°ƒæ¨¡å‹è¾“å‡º
            axes[1, 0].imshow(segmentation_map_finetuned)
            axes[1, 0].set_title(f"Finetuned CLIPSeg Output", fontsize=12)
            axes[1, 0].axis("off")
            
            # çœŸå®æ ‡ç­¾
            axes[1, 1].imshow(gt_img_bgr)
            axes[1, 1].set_title("Ground Truth", fontsize=12)
            axes[1, 1].axis("off")
            
            plt.tight_layout(pad=2.0)
            save_path = vis_dir / f"{image_path.stem}_4panel_comparison.png"
            plt.savefig(save_path, bbox_inches='tight')
            plt.close(fig)
            
        except Exception as e:
            print(f"å¤„ç†å›¾åƒ {image_path.name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            continue
    
    print(f"å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ°: {vis_dir}")

def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print("ğŸ”§ è¯„ä¼°é…ç½®:")
    print(f"  æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"  æµ‹è¯•æ•°æ®: {args.test_data}")
    print(f"  è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"  æ‰¹å¤„ç†å¤§å°: {args.batch_size}")
    print(f"  å¯è§†åŒ–æ ·æœ¬æ•°: {args.num_samples}")
    print()
    
    # è®¾ç½®è®¾å¤‡
    device = setup_device(args.device)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"è¯„ä¼°ç»“æœå°†ä¿å­˜åˆ°: {output_dir}")
    
    try:
        # åŠ è½½æ¨¡å‹
        print("åŠ è½½æ¨¡å‹...")
        model, processor, metadata = load_model(Path(args.model_path), device)
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        print("åŠ è½½æµ‹è¯•æ•°æ®...")
        test_loader, num_images = load_test_data(args.test_data, args.batch_size, processor)
        
        # è¯¦ç»†æ€§èƒ½è¯„ä¼°
        print("å¼€å§‹æ€§èƒ½è¯„ä¼°...")
        results = evaluate_detailed_performance(model, test_loader, device, URBAN_CLASSES)
        
        # ç”ŸæˆæŠ¥å‘Š
        print("ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
        report = generate_performance_report(results, URBAN_CLASSES, output_dir)
        
        # å¯è§†åŒ–é¢„æµ‹ç»“æœ
        print("ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
        visualize_predictions(args.model_path, args.test_data, device, URBAN_CLASSES, 
                             output_dir, args.num_samples)
        
        # æ‰“å°ç»“æœæ‘˜è¦
        print("\n" + "="*50)
        print("è¯„ä¼°ç»“æœæ‘˜è¦:")
        print("="*50)
        print(f"æ€»ä½“mIoU: {results['mean_iou']:.4f}")
        print("\nå„ç±»åˆ«IoU:")
        for cls, iou in results['class_ious'].items():
            print(f"  {cls}: {iou:.4f}")
        
        print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        
    except Exception as e:
        print(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())