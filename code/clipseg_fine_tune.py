import os
import random
import torch
import numpy as np
from PIL import Image
from glob import glob
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau
import torch.nn.functional as F
from tqdm import tqdm
from transformers import CLIPSegProcessor, CLIPSegForImageSegmentation
import cv2
from pathlib import Path
from torch.nn.utils.rnn import pad_sequence

# Disable HuggingFace Tokenizers parallelism warning
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ==============================================================================
# Configuration
# ==============================================================================
# Input directories (generated by prepare_finetune_data.py)
TRAIN_IMG_DIR = "./data/Vaihingen/finetune_data/train/images"
TRAIN_MASK_DIR = "./data/Vaihingen/finetune_data/train/masks"
VAL_IMG_DIR = "./data/Vaihingen/finetune_data/val/images"
VAL_MASK_DIR = "./data/Vaihingen/finetune_data/val/masks"

# Output directory
OUTPUT_DIR = "./clipseg_finetuned_model"

# Class definitions
CLASSES = ['impervious surface', 'building', 'low vegetation', 'tree', 'car', 'background']

# Training hyperparameters
PRETRAINED_MODEL = "CIDAS/clipseg-rd64-refined"
NUM_EPOCHS = 50
BATCH_SIZE = 4
LEARNING_RATE = 5e-5
PATIENCE = 5
DICE_WEIGHT = 0.8
NUM_WORKERS = 0  # For debugging on CPU, set num_workers to 0 for clearer error messages

# ==============================================================================
# Dataset Class
# ==============================================================================
class FineTuneDataset(Dataset):
    def __init__(self, image_paths, mask_dir, classes, processor):
        self.image_paths = image_paths
        self.mask_dir = mask_dir
        self.classes = classes
        self.processor = processor
        self.n_classes = len(classes)

    def __len__(self):
        return len(self.image_paths) * self.n_classes

    def __getitem__(self, idx):
        img_idx = idx // self.n_classes
        cls_idx = idx % self.n_classes

        image_path = self.image_paths[img_idx]
        cls_name = self.classes[cls_idx]
        
        image = Image.open(image_path).convert("RGB")

        base_fn = Path(image_path).stem
        safe_class_name = cls_name.replace(" ", "_").replace("/", "_")
        mask_path = os.path.join(self.mask_dir, f"{base_fn}_{safe_class_name}.png")
        
        if not os.path.isfile(mask_path):
            mask = np.zeros((352, 352), dtype=np.uint8)
        else:
            mask = np.array(Image.open(mask_path).convert("L"))

        # Note: padding=True only affects this call, does not pad across calls
        inputs = self.processor(
            text=[cls_name],
            images=[image],
            return_tensors="pt",
            padding=True 
        )
        
        mask_resized = cv2.resize(mask, (352, 352), interpolation=cv2.INTER_NEAREST)
        mask_tensor = torch.from_numpy((mask_resized > 127).astype(np.float32)).unsqueeze(0)

        return {
            "pixel_values": inputs.pixel_values.squeeze(0),
            "input_ids": inputs.input_ids.squeeze(0),
            "attention_mask": inputs.attention_mask.squeeze(0),
            "labels": mask_tensor
        }

# ==============================================================================
# Loss function and main training logic
# ==============================================================================
def dice_loss(logits, targets, eps=1e-7):
    preds = torch.sigmoid(logits)
    num = 2 * (preds * targets).sum(dim=[2, 3])
    den = (preds + targets).sum(dim=[2, 3]) + eps
    return (1 - (num / den)).mean()

def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    processor = CLIPSegProcessor.from_pretrained(PRETRAINED_MODEL)
    model = CLIPSegForImageSegmentation.from_pretrained(PRETRAINED_MODEL).to(device)

    # Custom collate_fn to handle variable-length text inputs
    def collate_fn(batch):
        pixel_values = torch.stack([item["pixel_values"] for item in batch])
        labels = torch.stack([item["labels"] for item in batch])
        input_ids = pad_sequence([item["input_ids"] for item in batch], batch_first=True, padding_value=processor.tokenizer.pad_token_id)
        attention_mask = pad_sequence([item["attention_mask"] for item in batch], batch_first=True, padding_value=0)
        return {
            "pixel_values": pixel_values,
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }

    # Create datasets and dataloaders
    train_image_paths = sorted(glob(os.path.join(TRAIN_IMG_DIR, "*.tif")))
    val_image_paths = sorted(glob(os.path.join(VAL_IMG_DIR, "*.tif")))

    train_dataset = FineTuneDataset(train_image_paths, TRAIN_MASK_DIR, CLASSES, processor)
    val_dataset = FineTuneDataset(val_image_paths, VAL_MASK_DIR, CLASSES, processor)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=collate_fn)

    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)
    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)

    best_val_loss = float('inf')
    patience_counter = 0

    print("üöÄ Start fine-tuning...")
    for epoch in range(1, NUM_EPOCHS + 1):
        model.train()
        train_loss_total = 0
        for batch in tqdm(train_loader, desc=f"Epoch {epoch} [Train]"):
            optimizer.zero_grad()
            pixel_values = batch["pixel_values"].to(device)
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            outputs = model(
                pixel_values=pixel_values,
                input_ids=input_ids,
                attention_mask=attention_mask
            )
            logits = outputs.logits.unsqueeze(1)

            bce = F.binary_cross_entropy_with_logits(logits, labels)
            dsc = dice_loss(logits, labels)
            loss = bce + DICE_WEIGHT * dsc
            
            loss.backward()
            optimizer.step()
            train_loss_total += loss.item()
        
        avg_train_loss = train_loss_total / len(train_loader)

        model.eval()
        val_loss_total = 0
        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f"Epoch {epoch} [Val]"):
                pixel_values = batch["pixel_values"].to(device)
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                labels = batch["labels"].to(device)

                outputs = model(
                    pixel_values=pixel_values,
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )
                logits = outputs.logits.unsqueeze(1)
                
                bce = F.binary_cross_entropy_with_logits(logits, labels)
                dsc = dice_loss(logits, labels)
                loss = bce + DICE_WEIGHT * dsc
                val_loss_total += loss.item()

        avg_val_loss = val_loss_total / len(val_loader)
        print(f"Epoch {epoch}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}")

        scheduler.step(avg_val_loss)

        if avg_val_loss < best_val_loss:
            print(f"  ‚ú® Validation loss decreased ({best_val_loss:.4f} -> {avg_val_loss:.4f}). Saving model...")
            best_val_loss = avg_val_loss
            patience_counter = 0
            model.save_pretrained(os.path.join(OUTPUT_DIR, "best_model"))
            processor.save_pretrained(os.path.join(OUTPUT_DIR, "best_model"))
        else:
            patience_counter += 1
            print(f"  Validation loss did not decrease. Patience: {patience_counter}/{PATIENCE}")
            if patience_counter >= PATIENCE:
                print("  ‚ö†Ô∏è Early stopping triggered! Ending training.")
                break
    
    print(f"\n‚úÖ Fine-tuning complete! Best model saved at: {os.path.join(OUTPUT_DIR, 'best_model')}")

if __name__ == "__main__":
    main()