import os
from glob import glob
from transformers import CLIPSegProcessor
from pathlib import Path
import json
from typing import Dict
import sys

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).resolve().parent.parent))

# å¯¼å…¥é…ç½®å’Œæ›´æ–°åçš„å·¥å…·å‡½æ•°
from config import *
from utils import (
    create_data_loader, create_model_and_optimizer, train_one_epoch,
    evaluate_model, save_model, set_seed, ensure_dirs, get_device,
    create_training_logger, update_training_log, save_training_log, get_current_lr
)

# ç¦ç”¨HuggingFaceè­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

def load_best_hyperparameters() -> Dict:
    """ä»æ–‡ä»¶åŠ è½½æœ€ä½³è¶…å‚æ•°ã€‚"""
    params_file = HYPERPARAMETER_SEARCH_DIR / "best_hyperparams.json"
    if not params_file.exists():
        raise FileNotFoundError(
            f"æœªæ‰¾åˆ°æœ€ä½³è¶…å‚æ•°æ–‡ä»¶: {params_file}\n"
            "è¯·å…ˆè¿è¡Œ 'hyperparameter_search.py' è„šæœ¬ã€‚"
        )
    
    with open(params_file, 'r') as f:
        best_params = json.load(f)
    
    print(f"âœ… æˆåŠŸåŠ è½½æœ€ä½³è¶…å‚æ•°: {best_params}")
    return best_params

def train_final_model(best_params: Dict):
    """ä½¿ç”¨æœ€ä½³å‚æ•°åœ¨å…¨é‡å¢å¼ºæ•°æ®ä¸Šè®­ç»ƒæœ€ç»ˆæ¨¡å‹ã€‚"""
    print("\nğŸ¯ å¼€å§‹è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
    
    device = get_device()
    processor = CLIPSegProcessor.from_pretrained(PRETRAINED_MODEL)
    
    # ä½¿ç”¨ç”± data_augmentation.py åˆ›å»ºçš„é¢„å…ˆåˆ†å‰²å¥½çš„æ•°æ®
    final_data_dir = FINETUNE_DATA_DIR / "cv_prepared_data" / "all_data_for_final_train"
    
    # åˆ†åˆ«åŠ è½½è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_data_dir = final_data_dir / "train"
    val_data_dir = final_data_dir / "val"
    
    train_images = sorted(glob(str(train_data_dir / "images/*.tif")))
    val_images = sorted(glob(str(val_data_dir / "images/*.tif")))
    
    train_mask_dir = str(train_data_dir / "masks")
    val_mask_dir = str(val_data_dir / "masks")
    
    if not train_images or not val_images:
        raise FileNotFoundError(
            f"æœªæ‰¾åˆ°é¢„å…ˆåˆ†å‰²çš„è®­ç»ƒ/éªŒè¯æ•°æ®ã€‚\n"
            f"è¯·å…ˆè¿è¡Œ data_augmentation.py ç”Ÿæˆæ•°æ®ã€‚\n"
            f"æœŸæœ›è·¯å¾„: {train_data_dir} å’Œ {val_data_dir}"
        )
    
    print(f"  è®­ç»ƒé›†: {len(train_images)} å¼ å›¾ç‰‡ï¼ˆå¢å¼ºåï¼‰")
    print(f"  éªŒè¯é›†: {len(val_images)} å¼ å›¾ç‰‡ï¼ˆåŸå§‹ï¼‰")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = create_data_loader(
        train_images, train_mask_dir, URBAN_CLASSES, processor,
        best_params['batch_size'], shuffle=True
    )
    
    val_loader = create_data_loader(
        val_images, val_mask_dir, URBAN_CLASSES, processor,
        best_params['batch_size'], shuffle=False
    )
    
    model, optimizer = create_model_and_optimizer(best_params['learning_rate'], device)
    
    # åˆ›å»ºè®­ç»ƒæ—¥å¿—è®°å½•å™¨
    final_logger = create_training_logger()
    final_logger['metadata']['model_type'] = 'final_model'
    final_logger['metadata']['hyperparameters'] = best_params
    final_logger['metadata']['train_images'] = len(train_images)
    final_logger['metadata']['val_images'] = len(val_images)
    final_logger['metadata']['data_source'] = str(final_data_dir)
    final_logger['metadata']['val_split_method'] = 'pre-split at original image level'
    
    best_val_loss = float('inf')
    best_epoch_model = None
    final_epochs = FINAL_TRAIN_CONFIG['num_epochs']
    patience = FINAL_TRAIN_CONFIG.get('patience', 10)
    min_delta = FINAL_TRAIN_CONFIG.get('min_delta', 1e-4)
    patience_counter = 0
    
    print(f"\n  æ—©åœè®¾ç½®: patience={patience}, min_delta={min_delta}")
    
    for epoch in range(1, final_epochs + 1):
        # è®­ç»ƒé˜¶æ®µ
        train_desc = f"æœ€ç»ˆè®­ç»ƒ Epoch {epoch}/{final_epochs} [è®­ç»ƒ]"
        train_loss = train_one_epoch(
            model, train_loader, optimizer, device,
            best_params.get('dice_weight', FINAL_TRAIN_CONFIG['default_dice_weight']),
            desc_str=train_desc
        )
        
        # éªŒè¯é˜¶æ®µ
        val_desc = f"æœ€ç»ˆè®­ç»ƒ Epoch {epoch}/{final_epochs} [éªŒè¯]"
        val_loss, val_iou = evaluate_model(
            model, val_loader, device,
            best_params.get('dice_weight', FINAL_TRAIN_CONFIG['default_dice_weight']),
            desc_str=val_desc
        )
        
        # æ›´æ–°è®­ç»ƒæ—¥å¿—
        current_lr = get_current_lr(optimizer)
        update_training_log(final_logger, epoch, train_loss, val_loss, current_lr)
        
        print(f"  Epoch {epoch}: è®­ç»ƒæŸå¤±={train_loss:.4f}, éªŒè¯æŸå¤±={val_loss:.4f}, éªŒè¯IoU={val_iou:.4f}")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss - min_delta:
            best_val_loss = val_loss
            best_epoch_model = epoch
            patience_counter = 0
            
            model_save_path = FINETUNED_MODEL_DIR / "best_model"
            
            # å‡†å¤‡è¦ä¿å­˜çš„å…ƒæ•°æ®
            metadata = {
                'best_train_loss': train_loss,
                'best_val_loss': val_loss,
                'best_val_iou': val_iou,
                'epoch': epoch,
                'hyperparameters': best_params,
                'total_epochs_planned': final_epochs
            }
            save_model(model, processor, model_save_path, metadata)
            print(f"  âœ¨ ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯æŸå¤±: {val_loss:.4f}, IoU: {val_iou:.4f})")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"\n  ğŸ›‘ æ—©åœè§¦å‘: éªŒè¯æŸå¤±å·²ç» {patience} è½®æ²¡æœ‰æ”¹å–„")
                final_logger['metadata']['early_stopped'] = True
                final_logger['metadata']['stopped_at_epoch'] = epoch
                break
    
    # ä¿å­˜è®­ç»ƒæ—¥å¿—
    final_logger['metadata']['best_model_saved_at_epoch'] = best_epoch_model
    log_path = FINETUNED_MODEL_DIR / "final_training_log.json"
    save_training_log(final_logger, log_path)
    
    print(f"\nâœ… æœ€ç»ˆæ¨¡å‹è®­ç»ƒå®Œæˆ! ")
    print(f"   - æ¨¡å‹ä¿å­˜åœ¨: {FINETUNED_MODEL_DIR / 'best_model'}")
    print(f"   - è®­ç»ƒæ—¥å¿—ä¿å­˜åœ¨: {log_path}")
    print(f"   - æœ€ä½³æ¨¡å‹æ¥è‡ª epoch {best_epoch_model}/{final_epochs}")

def main():
    """ä¸»å‡½æ•°ï¼šåŠ è½½å‚æ•° -> è®­ç»ƒæ¨¡å‹ -> ä¿å­˜æ¨¡å‹"""
    print("ğŸš€ æ­¥éª¤ 3: è®­ç»ƒæœ€ç»ˆæ¨¡å‹")
    print("=" * 60)
    
    set_seed()
    ensure_dirs()
    
    try:
        best_params = load_best_hyperparameters()
        train_final_model(best_params)
    except (FileNotFoundError, ValueError) as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        return

    print("\nğŸ‰ æœ€ç»ˆæ¨¡å‹è®­ç»ƒæµç¨‹æ‰§è¡Œå®Œæˆ!")

if __name__ == "__main__":
    main()
