import os
from glob import glob
from transformers import CLIPSegProcessor
from pathlib import Path
import json
from typing import Dict
import sys

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).resolve().parent.parent))

# å¯¼å…¥é…ç½®å’Œæ›´æ–°åçš„å·¥å…·å‡½æ•°
from config import *
from utils import (
    create_data_loader, create_model_and_optimizer, train_one_epoch,
    save_model, set_seed, ensure_dirs, get_device,
    create_training_logger, update_training_log, save_training_log, get_current_lr
)

# ç¦ç”¨HuggingFaceè­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

def load_best_hyperparameters() -> Dict:
    """ä»æ–‡ä»¶åŠ è½½æœ€ä½³è¶…å‚æ•°ã€‚"""
    params_file = HYPERPARAMETER_SEARCH_DIR / "best_hyperparams.json"
    if not params_file.exists():
        raise FileNotFoundError(
            f"æœªæ‰¾åˆ°æœ€ä½³è¶…å‚æ•°æ–‡ä»¶: {params_file}\n"
            "è¯·å…ˆè¿è¡Œ 'hyperparameter_search.py' è„šæœ¬ã€‚"
        )
    
    with open(params_file, 'r') as f:
        best_params = json.load(f)
    
    print(f"âœ… æˆåŠŸåŠ è½½æœ€ä½³è¶…å‚æ•°: {best_params}")
    return best_params

def train_final_model(best_params: Dict):
    """ä½¿ç”¨æœ€ä½³å‚æ•°åœ¨å…¨é‡å¢å¼ºæ•°æ®ä¸Šè®­ç»ƒæœ€ç»ˆæ¨¡å‹ã€‚"""
    print("\nğŸ¯ å¼€å§‹è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
    
    device = get_device()
    processor = CLIPSegProcessor.from_pretrained(PRETRAINED_MODEL)
    
    # ä½¿ç”¨ç”± data_augmentation.py åˆ›å»ºçš„å…¨é‡å¢å¼ºæ•°æ®
    final_data_dir = FINETUNE_DATA_DIR / "cv_prepared_data" / "all_data_for_final_train"
    all_augmented_images = glob(str(final_data_dir / "images/*.tif"))
    all_mask_dir = str(final_data_dir / "masks")

    if not all_augmented_images:
        raise FileNotFoundError(f"åœ¨ {final_data_dir} ä¸­æœªæ‰¾åˆ°ç”¨äºæœ€ç»ˆè®­ç»ƒçš„å›¾ç‰‡ã€‚")

    print(f"  æ€»è®­ç»ƒå›¾ç‰‡æ•°: {len(all_augmented_images)}")
    
    train_loader = create_data_loader(
        all_augmented_images, all_mask_dir, URBAN_CLASSES, processor,
        best_params['batch_size'], shuffle=True
    )
    
    model, optimizer = create_model_and_optimizer(best_params['learning_rate'], device)
    
    # åˆ›å»ºè®­ç»ƒæ—¥å¿—è®°å½•å™¨
    final_logger = create_training_logger()
    final_logger['metadata']['model_type'] = 'final_model'
    final_logger['metadata']['hyperparameters'] = best_params
    final_logger['metadata']['total_images'] = len(all_augmented_images)
    final_logger['metadata']['data_source'] = str(final_data_dir)
    
    best_loss = float('inf')
    best_epoch_model = None
    final_epochs = FINAL_TRAIN_CONFIG['num_epochs']
    
    for epoch in range(1, final_epochs + 1):
        # æ„é€ è¯¦ç»†çš„æè¿°ä¿¡æ¯
        train_desc = f"æœ€ç»ˆè®­ç»ƒ Epoch {epoch}/{final_epochs}"
        
        avg_loss = train_one_epoch(
            model, train_loader, optimizer, device,
            best_params.get('dice_weight', FINAL_TRAIN_CONFIG['default_dice_weight']),
            desc_str=train_desc
        )
        
        # æ›´æ–°è®­ç»ƒæ—¥å¿—
        current_lr = get_current_lr(optimizer)
        update_training_log(final_logger, epoch, avg_loss, lr=current_lr)
        
        if avg_loss < best_loss:
            best_loss = avg_loss
            best_epoch_model = epoch
            model_save_path = FINETUNED_MODEL_DIR / "best_model"
            
            # å‡†å¤‡è¦ä¿å­˜çš„å…ƒæ•°æ®
            metadata = {
                'best_loss': best_loss,
                'epoch': epoch,
                'hyperparameters': best_params,
                'total_epochs_planned': final_epochs
            }
            save_model(model, processor, model_save_path, metadata)
            print(f"  âœ¨ ä¿å­˜æœ€ä½³æ¨¡å‹ (æŸå¤±: {avg_loss:.4f})")
    
    # ä¿å­˜è®­ç»ƒæ—¥å¿—
    final_logger['metadata']['best_model_saved_at_epoch'] = best_epoch_model
    log_path = FINETUNED_MODEL_DIR / "final_training_log.json"
    save_training_log(final_logger, log_path)
    
    print(f"\nâœ… æœ€ç»ˆæ¨¡å‹è®­ç»ƒå®Œæˆ! ")
    print(f"   - æ¨¡å‹ä¿å­˜åœ¨: {FINETUNED_MODEL_DIR / 'best_model'}")
    print(f"   - è®­ç»ƒæ—¥å¿—ä¿å­˜åœ¨: {log_path}")
    print(f"   - æœ€ä½³æ¨¡å‹æ¥è‡ª epoch {best_epoch_model}/{final_epochs}")

def main():
    """ä¸»å‡½æ•°ï¼šåŠ è½½å‚æ•° -> è®­ç»ƒæ¨¡å‹ -> ä¿å­˜æ¨¡å‹"""
    print("ğŸš€ æ­¥éª¤ 3: è®­ç»ƒæœ€ç»ˆæ¨¡å‹")
    print("=" * 60)
    
    set_seed()
    ensure_dirs()
    
    try:
        best_params = load_best_hyperparameters()
        train_final_model(best_params)
    except (FileNotFoundError, ValueError) as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        return

    print("\nğŸ‰ æœ€ç»ˆæ¨¡å‹è®­ç»ƒæµç¨‹æ‰§è¡Œå®Œæˆ!")

if __name__ == "__main__":
    main()
