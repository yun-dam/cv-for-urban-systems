"""
ç®€å•çš„é…ç½®æ–‡ä»¶ - ä¸ºStanford UGVR CVé¡¹ç›®ç»Ÿä¸€ç®¡ç†å‚æ•°
é€‚åº¦æ”¹è¿›åŸå§‹ä»£ç ï¼Œä¿æŒç®€æ´æ˜“ç”¨
"""

import os
from pathlib import Path

# ==============================================================================
# é¡¹ç›®è·¯å¾„é…ç½®
# ==============================================================================
PROJECT_ROOT = Path(__file__).parent
DATA_DIR = PROJECT_ROOT / "data"
CODE_DIR = PROJECT_ROOT / "code"
MODELS_DIR = PROJECT_ROOT / "models"
OUTPUT_DIR = PROJECT_ROOT / "output"

# ==============================================================================
# æ•°æ®ç›¸å…³é…ç½®
# ==============================================================================
# Vaihingenæ•°æ®é›†è·¯å¾„
VAIHINGEN_DIR = DATA_DIR / "Vaihingen"
VAIHINGEN_IMAGES_DIR = VAIHINGEN_DIR / "top_cropped_512"
VAIHINGEN_LABELS_DIR = VAIHINGEN_DIR / "ground_truth_cropped_512"
FINETUNE_DATA_DIR = VAIHINGEN_DIR / "finetune_data"

# åŸå§‹å›¾ç‰‡æ± è·¯å¾„ï¼ˆç”¨äº5-fold CVï¼‰
ORIGINAL_IMG_DIR = FINETUNE_DATA_DIR / "finetune_pool" / "images"
ORIGINAL_MASK_DIR = FINETUNE_DATA_DIR / "finetune_pool" / "masks"

# ==============================================================================
# æ¨¡å‹ç›¸å…³é…ç½®
# ==============================================================================
# CLIPSegé¢„è®­ç»ƒæ¨¡å‹
PRETRAINED_MODEL = "CIDAS/clipseg-rd64-refined"

# åŸå¸‚åˆ†å‰²ç±»åˆ«
URBAN_CLASSES = [
    'impervious surface', 
    'building', 
    'low vegetation', 
    'tree', 
    'car', 
    'background'
]

# ==============================================================================
# æ•°æ®å¤„ç†å‚æ•°é…ç½®
# ==============================================================================
# æ•°æ®é›†å‚æ•°
CROP_SIZE = 512
NUM_ORIGINAL_IMAGES = 20  # åŸå§‹å›¾ç‰‡æ•°é‡ï¼ˆç”¨äº5-fold CVï¼‰
N_CV_FOLDS = 5           # äº¤å‰éªŒè¯foldæ•°

# æ•°æ®å¢å¼ºå‚æ•°
AUGMENTATION_FACTOR = 5   # æ¯å¼ å›¾ç‰‡çš„å¢å¼ºå€æ•°
AUGMENTATION_CONFIG = {
    'horizontal_flip': True,
    'vertical_flip': True,
    'rotation_range': (-45, 45),
    'brightness_limit': 0.2,
    'contrast_limit': 0.2,
    'gaussian_noise_var': (10.0, 50.0),
    'blur_limit': 3,
}

# ==============================================================================
# è®­ç»ƒå‚æ•°é…ç½®
# ==============================================================================
# åŸºç¡€è®­ç»ƒå‚æ•°ï¼ˆç”¨äºæœ€ç»ˆè®­ç»ƒï¼‰
TRAIN_CONFIG = {
    'num_epochs': 50,
    'batch_size': 4,
    'learning_rate': 5e-5,
    'patience': 5,              # æ—©åœè€å¿ƒå€¼
    'min_delta': 0.001,         # æ—©åœæœ€å°æ”¹è¿›é˜ˆå€¼
    'dice_weight': 0.8,         # Dice lossæƒé‡
    'bce_weight': 0.2,          # BCE lossæƒé‡
    'gradient_clip_val': 1.0,   # æ¢¯åº¦è£å‰ªå€¼
    'lr_scheduler': 'cosine',   # å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹
    'warmup_epochs': 2,         # çƒ­èº«epochæ•°
}

# ==============================================================================
# è¶…å‚æ•°æœç´¢ç©ºé—´é…ç½®
# ==============================================================================
HYPERPARAMETER_SEARCH_SPACE = {
    # å­¦ä¹ ç‡æœç´¢èŒƒå›´
    'learning_rate': {
        'type': 'loguniform',
        'low': 1e-6,
        'high': 1e-3,
    },
    
    # Dice lossæƒé‡æœç´¢èŒƒå›´
    'dice_weight': {
        'type': 'uniform',
        'low': 0.6,
        'high': 0.9,
    },
    
    # æ‰¹æ¬¡å¤§å°é€‰é¡¹ï¼ˆé’ˆå¯¹ç¬”è®°æœ¬ç”µè„‘ä¼˜åŒ–ï¼‰
    'batch_size': {
        'type': 'categorical',
        'choices': [2, 4, 8],
    },
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹
    'lr_scheduler': {
        'type': 'categorical',
        'choices': ['cosine', 'step', 'exponential'],
    },
    
    # æ¢¯åº¦è£å‰ªå€¼
    'gradient_clip_val': {
        'type': 'categorical',
        'choices': [0.5, 1.0, 2.0],
    },
}

# Optunaè¶…å‚æ•°æœç´¢é…ç½®
OPTUNA_CONFIG = {
    'n_trials': 20,              # æœç´¢è¯•éªŒæ¬¡æ•°
    'n_jobs': 1,                 # å¹¶è¡Œä»»åŠ¡æ•°ï¼ˆç¬”è®°æœ¬ç”µè„‘å»ºè®®1ï¼‰
    'study_name': 'clipseg_cv_search',
    'direction': 'maximize',     # ä¼˜åŒ–æ–¹å‘ï¼ˆæœ€å¤§åŒ–IoUï¼‰
    'sampler': 'TPE',           # é‡‡æ ·å™¨ç±»å‹
    'pruner': 'median',         # å‰ªæç­–ç•¥
}

# ==============================================================================
# ç³»ç»Ÿå‚æ•°é…ç½®
# ==============================================================================
# ç³»ç»Ÿå‚æ•°
NUM_WORKERS = 0  # æ•°æ®åŠ è½½å™¨çº¿ç¨‹æ•°ï¼ˆè®¾ä¸º0ä¾¿äºè°ƒè¯•ï¼‰
RANDOM_SEED = 42
DEVICE = 'auto'  # 'auto', 'cuda', 'cpu', 'mps'

# æ—¥å¿—é…ç½®
LOG_CONFIG = {
    'log_level': 'INFO',
    'log_dir': OUTPUT_DIR / 'logs',
    'tensorboard': True,
    'wandb': False,  # å¯é€‰çš„wandbé›†æˆ
}

# ==============================================================================
# è¾“å‡ºè·¯å¾„é…ç½®
# ==============================================================================
# æ¨¡å‹ä¿å­˜è·¯å¾„
FINETUNED_MODEL_DIR = MODELS_DIR / "clipseg_finetuned"
HYPERPARAMETER_SEARCH_DIR = MODELS_DIR / "hyperparameter_search"

# ç»“æœè¾“å‡ºè·¯å¾„
TRAINING_OUTPUT_DIR = OUTPUT_DIR / "training"
EVALUATION_OUTPUT_DIR = OUTPUT_DIR / "evaluation"
COMPARISON_OUTPUT_DIR = OUTPUT_DIR / "model_comparison"

# ==============================================================================
# å®ç”¨å‡½æ•°
# ==============================================================================
def ensure_dirs():
    """ç¡®ä¿æ‰€æœ‰å¿…è¦çš„ç›®å½•å­˜åœ¨"""
    dirs_to_create = [
        DATA_DIR, MODELS_DIR, OUTPUT_DIR,
        FINETUNED_MODEL_DIR, HYPERPARAMETER_SEARCH_DIR,
        TRAINING_OUTPUT_DIR, EVALUATION_OUTPUT_DIR, COMPARISON_OUTPUT_DIR
    ]
    
    for dir_path in dirs_to_create:
        dir_path.mkdir(parents=True, exist_ok=True)
    
    print(f"âœ… å·²ç¡®ä¿æ‰€æœ‰ç›®å½•å­˜åœ¨")

def get_device():
    """è·å–å¯ç”¨çš„è®¾å¤‡"""
    import torch
    
    if DEVICE == 'auto':
        if torch.cuda.is_available():
            device = torch.device("cuda")
            print(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
        elif torch.backends.mps.is_available():
            device = torch.device("mps")
            print("âœ… ä½¿ç”¨Apple Silicon GPU (MPS)")
        else:
            device = torch.device("cpu")
            print("âš ï¸ ä½¿ç”¨CPUè¿›è¡Œè®­ç»ƒ")
    else:
        device = torch.device(DEVICE)
        print(f"âœ… ä½¿ç”¨æŒ‡å®šè®¾å¤‡: {DEVICE}")
    
    return device

def set_seed(seed=RANDOM_SEED):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§"""
    import random
    import numpy as np
    import torch
    
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    
    print(f"âœ… è®¾ç½®éšæœºç§å­: {seed}")

def get_augmentation_transform():
    """è·å–æ•°æ®å¢å¼ºå˜æ¢ï¼ŒåŸºäºé…ç½®æ–‡ä»¶"""
    import albumentations as A
    
    transforms = []
    
    if AUGMENTATION_CONFIG.get('horizontal_flip'):
        transforms.append(A.HorizontalFlip(p=0.5))
    
    if AUGMENTATION_CONFIG.get('vertical_flip'):
        transforms.append(A.VerticalFlip(p=0.5))
    
    if 'rotation_range' in AUGMENTATION_CONFIG:
        min_angle, max_angle = AUGMENTATION_CONFIG['rotation_range']
        transforms.append(A.Rotate(limit=(min_angle, max_angle), p=0.5))
    
    if 'brightness_limit' in AUGMENTATION_CONFIG:
        transforms.append(A.RandomBrightnessContrast(
            brightness_limit=AUGMENTATION_CONFIG['brightness_limit'],
            contrast_limit=AUGMENTATION_CONFIG.get('contrast_limit', 0.2),
            p=0.5
        ))
    
    if 'gaussian_noise_var' in AUGMENTATION_CONFIG:
        var_limit = AUGMENTATION_CONFIG['gaussian_noise_var']
        transforms.append(A.GaussNoise(var_limit=var_limit, p=0.3))
    
    if 'blur_limit' in AUGMENTATION_CONFIG:
        transforms.append(A.GaussianBlur(blur_limit=AUGMENTATION_CONFIG['blur_limit'], p=0.3))
    
    return A.Compose(transforms)

def get_hyperparameter_from_trial(trial, param_name):
    """ä»Optuna trialä¸­è·å–è¶…å‚æ•°"""
    param_config = HYPERPARAMETER_SEARCH_SPACE[param_name]
    
    if param_config['type'] == 'loguniform':
        return trial.suggest_loguniform(param_name, param_config['low'], param_config['high'])
    elif param_config['type'] == 'uniform':
        return trial.suggest_uniform(param_name, param_config['low'], param_config['high'])
    elif param_config['type'] == 'categorical':
        return trial.suggest_categorical(param_name, param_config['choices'])
    else:
        raise ValueError(f"Unknown parameter type: {param_config['type']}")

def save_config(config_dict, filename):
    """ä¿å­˜é…ç½®åˆ°JSONæ–‡ä»¶"""
    import json
    from pathlib import Path
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_path = Path(filename)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # è½¬æ¢Pathå¯¹è±¡ä¸ºå­—ç¬¦ä¸²
    def convert_paths(obj):
        if isinstance(obj, Path):
            return str(obj)
        elif isinstance(obj, dict):
            return {k: convert_paths(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_paths(item) for item in obj]
        return obj
    
    config_to_save = convert_paths(config_dict)
    
    with open(output_path, 'w') as f:
        json.dump(config_to_save, f, indent=4)
    
    print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {output_path}")

if __name__ == "__main__":
    # è¿è¡Œé…ç½®æµ‹è¯•
    print("ğŸ”§ é…ç½®æ–‡ä»¶æµ‹è¯•")
    print(f"é¡¹ç›®æ ¹ç›®å½•: {PROJECT_ROOT}")
    print(f"æ•°æ®ç›®å½•: {DATA_DIR}")
    print(f"æ¨¡å‹ç›®å½•: {MODELS_DIR}")
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    
    print("\nğŸ“Š æ•°æ®å¤„ç†é…ç½®:")
    print(f"åŸå§‹å›¾ç‰‡æ•°é‡: {NUM_ORIGINAL_IMAGES}")
    print(f"äº¤å‰éªŒè¯æŠ˜æ•°: {N_CV_FOLDS}")
    print(f"æ•°æ®å¢å¼ºå€æ•°: {AUGMENTATION_FACTOR}")
    
    print("\nğŸ” è¶…å‚æ•°æœç´¢é…ç½®:")
    print(f"æœç´¢è¯•éªŒæ¬¡æ•°: {OPTUNA_CONFIG['n_trials']}")
    print(f"æœç´¢å‚æ•°: {list(HYPERPARAMETER_SEARCH_SPACE.keys())}")
    
    ensure_dirs()
    print("\nâœ… é…ç½®æ–‡ä»¶æµ‹è¯•å®Œæˆ")